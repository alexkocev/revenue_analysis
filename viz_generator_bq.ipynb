{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -- IMPORTS --\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = 'notebook' # viz outputs in the notebook\n",
    "\n",
    "# -- M2 DATA IMPORT --\n",
    "file_path = \"data/bq_magento.csv\"  \n",
    "data_m2 = pd.read_csv(file_path)\n",
    "data_m2['datetime'] = pd.to_datetime(data_m2['datetime'])\n",
    "data_m2 = data_m2.drop(columns=['quantity'])\n",
    "\n",
    "\n",
    "# -- GA4 DATA IMPORT --\n",
    "file_path = \"data/bq_ga4.csv\"   \n",
    "data_ga4 = pd.read_csv(file_path)\n",
    "data_ga4['datetime'] = pd.to_datetime(data_ga4['datetime'])\n",
    "\n",
    "# -- Build GA4 Source table --\n",
    "data_ga4_source = data_ga4.groupby(\n",
    "    ['transaction_id', 'datetime']\n",
    ").agg(\n",
    "    traffic_source=('traffic_source', 'first'),        \n",
    "    traffic_medium=('traffic_medium', 'first'),        \n",
    "    traffic_name=('traffic_name', 'first'),          \n",
    "    revenue=('revenue', 'first'),       \n",
    "    quantity=('quantity', 'first')  \n",
    ").reset_index()\n",
    "# Capitalize traffic source and replace (direct)\n",
    "data_ga4_source['traffic_source'] = data_ga4_source['traffic_source'].str.capitalize()\n",
    "data_ga4_source['traffic_source'] = data_ga4_source['traffic_source'].replace('(direct)', 'Direct')\n",
    "\n",
    "# -- Build GA4 Item table --\n",
    "data_ga4_item = data_ga4[['transaction_id', 'datetime', 'item_name', \n",
    "                          'category_name1', 'category_name2', 'category_name3', 'category_name4', 'category_name5', \n",
    "                          'item_revenue', 'item_quantity' ]].copy()\n",
    "# Lowercase and title the category columns\n",
    "for col in [\"category_name1\", \"category_name2\", \"category_name3\", \"category_name4\"]:\n",
    "    data_ga4_item[col] = data_ga4_item[col].str.lower()\n",
    "for col in [\"category_name1\", \"category_name2\", \"category_name3\", \"category_name4\"]:\n",
    "    data_ga4_item[col] = data_ga4_item[col].str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNIFY USERS ie. create user_id --\n",
    "def clean_name_address(text: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Lowercase\n",
    "    2) Remove all NON-alphanumeric characters (anything except letters, digits, and spaces),\n",
    "       replacing them with a single space\n",
    "    3) Replace multiple spaces with a single space\n",
    "    4) Strip leading/trailing spaces\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if pd.notnull(text) else \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Replace any sequence of non-alphanumeric (including punctuation) with a space\n",
    "    # Here [^a-z0-9 ] means \"any char that is NOT (a to z or 0-9 or space)\".\n",
    "    text = re.sub(r'[^a-z0-9 ]+', ' ', text)\n",
    "    \n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_email(text: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Lowercase\n",
    "    2) Strip leading/trailing spaces\n",
    "    \n",
    "    We do NOT remove non-alphanumeric chars from email because\n",
    "    valid addresses may contain '.', '+', '_', etc. \n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if pd.notnull(text) else \"\"\n",
    "    \n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def generate_user_id(df):\n",
    "    user_mapping = {}\n",
    "    user_counter = 1\n",
    "    user_ids = []\n",
    "    \n",
    "    df['user_name'] = df['user_name'].apply(clean_name_address)\n",
    "    df['shipping_address'] = df['shipping_address'].apply(clean_name_address)\n",
    "    df['email'] = df['email'].apply(clean_email)\n",
    "        \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Users\"):\n",
    " \n",
    "        email, name, address = row['email'], row['user_name'], row['shipping_address']\n",
    "\n",
    "        # Priority: Same email = same user\n",
    "        if email in user_mapping:\n",
    "            user_ids.append(user_mapping[email])\n",
    "        \n",
    "        # Otherwise, check for same name + same address\n",
    "        elif (name, address) in user_mapping:\n",
    "            user_ids.append(user_mapping[(name, address)])\n",
    "        \n",
    "        # Otherwise, create a new user_id\n",
    "        else:\n",
    "            user_mapping[email] = user_counter\n",
    "            user_mapping[(name, address)] = user_counter\n",
    "            user_ids.append(user_counter)\n",
    "            user_counter += 1\n",
    "    \n",
    "    df['user_id'] = pd.Series(user_ids, dtype=str)\n",
    "    return df\n",
    "\n",
    "data_m2 = generate_user_id(data_m2)\n",
    "\n",
    "# Merge data_m2 and data_ga4_item using a left join on the 'transaction_id' column\n",
    "data_ga4_item = pd.merge(data_ga4_item, data_m2, on='transaction_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATION\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# -- Viz currencies --\n",
    "data_m2.groupby(['currency']).agg(revenue=('revenue', 'sum')) \\\n",
    "    .assign(percent_revenue=lambda x: ((x['revenue'] / x['revenue'].sum()) * 100).round(1)) \\\n",
    "    .sort_values(by='revenue', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Choose the Currency for the analysis -------------------------------\n",
    "currency_to_choose = 'EUR'\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# -- Viz top emails --\n",
    "\n",
    "emails = data_m2.groupby('email')['transaction_id'].count()\n",
    "emails = emails.sort_values(ascending=False)\n",
    "emails.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Remove wrong or internal emails -------------------------------\n",
    "email_keywords_to_ignore = ['qwertee', 'test']\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "pattern = '|'.join(email_keywords_to_ignore)\n",
    "data_m2 = data_m2[~data_m2['user_id'].str.contains(pattern, case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -- Viz top categories --\n",
    "\n",
    "# Total unique transactions\n",
    "total_txn = data_ga4_item[\"transaction_id\"].nunique()\n",
    "\n",
    "# Helper function to get percentage of transactions per category\n",
    "def category_percentages(df, col):\n",
    "    counts = df.groupby(col)[\"transaction_id\"].nunique().sort_values(ascending=False)\n",
    "    pct = (counts / total_txn) * 100\n",
    "    pct = pct.round(1)  # Round to 1 decimal\n",
    "    pct = pct.astype(str) + \"%\"  # Add the '%' sign\n",
    "    return pct.reset_index(name=\"percentage_of_transactions\")\n",
    "\n",
    "# For each category column, compute + sort by percentage\n",
    "cat1_df = category_percentages(data_ga4_item, \"category_name1\")\n",
    "cat2_df = category_percentages(data_ga4_item, \"category_name2\")\n",
    "cat3_df = category_percentages(data_ga4_item, \"category_name3\")\n",
    "cat4_df = category_percentages(data_ga4_item, \"category_name4\")\n",
    "\n",
    "cat1_df.columns = [\"category_name1\", \"pct1\"]\n",
    "cat2_df.columns = [\"category_name2\", \"pct2\"]\n",
    "cat3_df.columns = [\"category_name3\", \"pct3\"]\n",
    "cat4_df.columns = [\"category_name4\", \"pct4\"]\n",
    "\n",
    "categ_df = pd.concat(\n",
    "    [cat1_df.reset_index(drop=True),\n",
    "     cat2_df.reset_index(drop=True),\n",
    "     cat3_df.reset_index(drop=True),\n",
    "     cat4_df.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "categ_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Keep category level that makes sense ---------------------------------\n",
    "category_to_analyse = \"category_name1\"\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -- Viz top order status --\n",
    "\n",
    "# Lowercase the order_status (optional)\n",
    "data_m2[\"order_status\"] = data_m2[\"order_status\"].str.lower()\n",
    "# Calculate total unique transactions\n",
    "total_txn = data_m2[\"transaction_id\"].nunique()\n",
    "# Group by order_status:\n",
    "grouped = data_m2.groupby(\"order_status\").agg(\n",
    "    total_revenue=(\"revenue\", \"sum\"),\n",
    "    unique_transactions=(\"transaction_id\", \"nunique\")\n",
    ").reset_index()\n",
    "# Compute percentage of unique transactions\n",
    "grouped[\"percentage_of_transactions\"] = (\n",
    "    grouped[\"unique_transactions\"] / total_txn * 100\n",
    ")\n",
    "# Sort descending by percentage of transactions\n",
    "order_status_df = grouped.sort_values(\"percentage_of_transactions\", ascending=False)\n",
    "\n",
    "order_status_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Remmove order status that show refunds or cancelations ---------------------\n",
    "order_status_keyWords_Refund_Cancel = [\"cancel\", \"refund\"]\n",
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# -- Viz date range --\n",
    "print(f\"Source: Magento 2 from {data_m2['datetime'].min().strftime('%b %d, %Y')} to {data_m2['datetime'].max().strftime('%b %d, %Y')}\")\n",
    "print(f\"Source: GA4 from {data_ga4_source['datetime'].min().strftime('%b %d, %Y')} to {data_ga4_source['datetime'].max().strftime('%b %d, %Y')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Convert multiple currency revenue to one --\n",
    "\"\"\"\n",
    "Converts the revenue column in the dataframe to the specified base currency using historical exchange rates.\n",
    "Rounds the converted revenue to 2 decimals.\n",
    "\"\"\"\n",
    "# Create date column for merging\n",
    "data_m2['date'] = data_m2['datetime'].dt.date\n",
    "\n",
    "# Get min and max dates from the data (with a small buffer)\n",
    "start_date = data_m2['datetime'].min() - pd.Timedelta(days=7)\n",
    "end_date = data_m2['datetime'].max() + pd.Timedelta(days=7)\n",
    "\n",
    "print(f\"Data date range: {data_m2['datetime'].min()} to {data_m2['datetime'].max()}\")\n",
    "print(f\"Fetching exchange rates from {start_date} to {end_date}\")\n",
    "\n",
    "# Get unique currencies in the dataframe\n",
    "unique_currencies = data_m2['currency'].unique()\n",
    "print(f\"Unique currencies found: {unique_currencies}\")\n",
    "\n",
    "# Dictionary to store exchange rate data for each currency pair\n",
    "exchange_rates_dict = {}\n",
    "\n",
    "# Fetch historical exchange rates for each currency to the base currency\n",
    "for currency in unique_currencies:\n",
    "    if currency == currency_to_choose:\n",
    "        # If the currency is the base currency, set rate = 1\n",
    "        exchange_rates_dict[currency] = pd.Series(1, index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    " \n",
    "    else:\n",
    "        currency_pair = currency + currency_to_choose + '=X'\n",
    "        print(f\"Fetching exchange rates for: {currency_pair}\")\n",
    "\n",
    "        # Download historical exchange rates\n",
    "        rates_df = yf.download(\n",
    "            currency_pair, \n",
    "            start=start_date.strftime('%Y-%m-%d'), \n",
    "            end=end_date.strftime('%Y-%m-%d'), \n",
    "            interval='1d'\n",
    "        )['Close']\n",
    "        \n",
    "        # Convert index to date for merging\n",
    "        rates_df.index = rates_df.index.date\n",
    "        exchange_rates_dict[currency] = rates_df\n",
    "\n",
    "# Create a result dataframe\n",
    "data_m2['revenue_converted'] = data_m2['revenue']  # Initialize with original values\n",
    "\n",
    "# Ensure 'date' column in data_m2 is in datetime format\n",
    "data_m2['date'] = pd.to_datetime(data_m2['date'])\n",
    "\n",
    "# Loop through exchange rates and update the 'rate' column\n",
    "for currency, rates in exchange_rates_dict.items():\n",
    "    rates.index = pd.to_datetime(rates.index)\n",
    "    rates = rates.reindex(pd.date_range(start=rates.index.min(), end=rates.index.max(), freq='D')).ffill()\n",
    "    rate_mapping = rates.squeeze().to_dict()\n",
    "    \n",
    "    mask = data_m2['currency'] == currency\n",
    "    data_m2.loc[mask, 'rate'] = data_m2.loc[mask, 'date'].map(rate_mapping)\n",
    "\n",
    "# If the currency is the base currency, set the rate to 1\n",
    "mask_base_currency = data_m2['currency'] == currency_to_choose\n",
    "data_m2.loc[mask_base_currency, 'rate'] = 1\n",
    "\n",
    "data_m2['revenue_converted'] = data_m2['revenue'] * data_m2['rate']\n",
    "\n",
    "# Keep only usefull columns\n",
    "data_m2 = data_m2[['transaction_id', 'user_id', 'datetime', 'revenue_converted', 'discount', 'order_status']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1.1 REVENUE TIMELINE CHART (Grouped by Month) --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Ensure the datetime column is in the correct format\n",
    "data_m2_noCancel_noRefund['datetime'] = pd.to_datetime(data_m2_noCancel_noRefund['datetime'])\n",
    "\n",
    "# Group the data by month and sum the revenue\n",
    "# Use the 'datetime' column and extract the month and year to group by\n",
    "revenue_timeline = data_m2_noCancel_noRefund.groupby(data_m2_noCancel_noRefund['datetime'].dt.to_period('M')).agg({'revenue_converted': 'sum'}).reset_index()\n",
    "\n",
    "# Convert period to a regular datetime for better plotting\n",
    "revenue_timeline['datetime'] = revenue_timeline['datetime'].dt.to_timestamp()\n",
    "\n",
    "# AFTER you've created revenue_timeline, remove the last row if it corresponds to the current (incomplete) month:\n",
    "current_month = pd.Timestamp.now().month\n",
    "current_year = pd.Timestamp.now().year\n",
    "\n",
    "# Check if the last row in revenue_timeline is from the current month/year, and drop if so\n",
    "if (revenue_timeline.iloc[-1]['datetime'].month == current_month and\n",
    "    revenue_timeline.iloc[-1]['datetime'].year == current_year):\n",
    "    revenue_timeline = revenue_timeline.iloc[:-1]\n",
    "    \n",
    "# Create the Plotly figure\n",
    "fig_revenue_timeline = go.Figure()\n",
    "\n",
    "# Add a trace for the revenue timeline\n",
    "fig_revenue_timeline.add_trace(go.Scatter(\n",
    "    x=revenue_timeline['datetime'],  \n",
    "    y=revenue_timeline['revenue_converted'],  \n",
    "    mode='lines',                 \n",
    "    name='Revenue Over Time',\n",
    "    line=dict(color='lightblue')  \n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_revenue_timeline.update_layout(\n",
    "    title='Revenue Over Time (Grouped by Month)',\n",
    "    xaxis_title='',\n",
    "    yaxis_title='Total Revenue ($)',\n",
    "    template='plotly_white',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_revenue_timeline.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1.2 REVENUE TIMELINE CHART (Grouped by Month) --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Ensure the datetime column is in the correct format\n",
    "data_m2_noCancel_noRefund.loc[:, 'datetime'] = pd.to_datetime(data_m2_noCancel_noRefund['datetime'])\n",
    "\n",
    "\n",
    "# -- MONTH Data Preparation --\n",
    "# Extract month and group by month, summing the revenue for each month\n",
    "data_m2_noCancel_noRefund.loc[:, 'month'] = data_m2_noCancel_noRefund['datetime'].dt.month\n",
    "\n",
    "# Group by month and sum the revenue\n",
    "revenue_by_month = data_m2_noCancel_noRefund.groupby('month').agg({'revenue_converted': 'sum'}).reset_index()\n",
    "\n",
    "# Calculate the total revenue\n",
    "total_revenue = revenue_by_month['revenue_converted'].sum()\n",
    "\n",
    "# Calculate the revenue percentage for each month\n",
    "revenue_by_month['revenue_percentage'] = (revenue_by_month['revenue_converted'] / total_revenue) * 100\n",
    "\n",
    "# Handle the monthly average revenue\n",
    "monthly_average = revenue_by_month['revenue_percentage'].mean()\n",
    "\n",
    "# Map month number to month name\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "revenue_by_month['month_name'] = revenue_by_month['month'].apply(lambda x: month_names[x-1])\n",
    "\n",
    "\n",
    "\n",
    "# -- WEEK Data Preparation --\n",
    "# Extract the weekday (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
    "data_m2_noCancel_noRefund.loc[:, 'weekday'] = data_m2_noCancel_noRefund['datetime'].dt.weekday\n",
    "\n",
    "# Group by weekday and sum the revenue for each weekday (now including Saturday and Sunday)\n",
    "revenue_by_weekday = data_m2_noCancel_noRefund.groupby('weekday').agg({'revenue_converted': 'sum'}).reset_index()\n",
    "\n",
    "# Calculate the total revenue\n",
    "total_revenue = revenue_by_weekday['revenue_converted'].sum()\n",
    "\n",
    "# Calculate the revenue percentage for each weekday\n",
    "revenue_by_weekday['revenue_percentage'] = (revenue_by_weekday['revenue_converted'] / total_revenue) * 100\n",
    "\n",
    "# Handle the weekday average revenue percentage\n",
    "weekday_average = revenue_by_weekday['revenue_percentage'].mean()\n",
    "\n",
    "# Map weekday number to weekday name (including Saturday and Sunday)\n",
    "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "revenue_by_weekday['weekday_name'] = revenue_by_weekday['weekday'].apply(lambda x: weekday_names[x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create subplots with shared y-axis\n",
    "fig_revenue_distributin_month_and_weekday = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Monthly Revenue Percentage', 'Weekday Revenue Percentage'),\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Add monthly revenue bars to the left subplot\n",
    "fig_revenue_distributin_month_and_weekday.add_trace(\n",
    "    go.Bar(\n",
    "        x=revenue_by_month['month_name'],\n",
    "        y=revenue_by_month['revenue_percentage'],\n",
    "        name='Monthly Revenue',\n",
    "        marker=dict(color='lightblue'),\n",
    "        showlegend=False  # Hide legend\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add monthly average line to the left subplot\n",
    "fig_revenue_distributin_month_and_weekday.add_trace(\n",
    "    go.Scatter(\n",
    "        x=revenue_by_month['month_name'],\n",
    "        y=[monthly_average] * len(revenue_by_month),\n",
    "        mode='lines',\n",
    "        name='Monthly Average',\n",
    "        line=dict(color='blue', dash='dash'),\n",
    "        showlegend=False  # Hide legend\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add weekday revenue bars to the right subplot\n",
    "fig_revenue_distributin_month_and_weekday.add_trace(\n",
    "    go.Bar(\n",
    "        x=revenue_by_weekday['weekday_name'],\n",
    "        y=revenue_by_weekday['revenue_percentage'],\n",
    "        name='Weekday Revenue',\n",
    "        marker=dict(color='#75B7E5'),\n",
    "        showlegend=False  # Hide legend\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add weekday average line to the right subplot\n",
    "fig_revenue_distributin_month_and_weekday.add_trace(\n",
    "    go.Scatter(\n",
    "        x=revenue_by_weekday['weekday_name'],\n",
    "        y=[weekday_average] * len(revenue_by_weekday),\n",
    "        mode='lines',\n",
    "        name='Weekday Average',\n",
    "        line=dict(color='blue', dash='dash'),\n",
    "        showlegend=False  # Hide legend\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig_revenue_distributin_month_and_weekday.update_layout(\n",
    "    showlegend=False,  # Hide legend\n",
    "    template='plotly_white',\n",
    "    width=1200,\n",
    "    height=500,\n",
    "    yaxis_title='Revenue Percentage (%)',\n",
    "    margin=dict(t=30)  # Reduce top margin since we removed the title\n",
    ")\n",
    "\n",
    "# Update x-axis properties for both subplots\n",
    "fig_revenue_distributin_month_and_weekday.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig_revenue_distributin_month_and_weekday.update_xaxes(tickangle=45, row=1, col=2)\n",
    "\n",
    "# Show the figure\n",
    "fig_revenue_distributin_month_and_weekday.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2. CLIENT REPARTITION CHART --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Group by customer and sum the revenue (Grand Total)\n",
    "client_revenue = data_m2_noCancel_noRefund.groupby('user_id')['revenue_converted'].sum()\n",
    "\n",
    "# Sort by revenue in descending order\n",
    "client_revenue = client_revenue.sort_values(ascending=False)\n",
    "\n",
    "# Calculate the percentage of revenue per client\n",
    "client_revenue_percentage = client_revenue / client_revenue.sum()\n",
    "\n",
    "# Limit the data to top 1000 clients\n",
    "client_revenue_percentage = client_revenue_percentage[:1000]\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig_client_revenue = go.Figure()\n",
    "\n",
    "# Use a bar chart instead of a line chart\n",
    "fig_client_revenue.add_trace(go.Bar(\n",
    "    x=list(range(1, len(client_revenue_percentage) + 1)),  # x-axis: clients ordered by revenue\n",
    "    y=client_revenue_percentage * 100,  # y-axis: the percentage of revenue by each client\n",
    "    name='% of Revenue by Client',\n",
    "    marker=dict(color='blue')  # Set color to blue\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_client_revenue.update_layout(\n",
    "    title='Revenue by Client',\n",
    "    xaxis_title='Clients Ordered by Revenue',\n",
    "    yaxis_title='% of Revenue (Lifetime Value)',\n",
    "    yaxis=dict(tickformat='.0f%'),\n",
    "    template='plotly_white',\n",
    "    width=900,\n",
    "    height=500,\n",
    "    bargap=0\n",
    ")\n",
    "\n",
    "fig_client_revenue.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3. CLIENT REPARTITION --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "# Group and sort revenues by customer\n",
    "client_revenue = data_m2_noCancel_noRefund.groupby('user_id')['revenue_converted'].sum()\n",
    "client_revenue_sorted = client_revenue.sort_values(ascending=False)\n",
    "\n",
    "# Calculate total\n",
    "total_revenue = client_revenue_sorted.sum()\n",
    "total_clients = len(client_revenue_sorted)\n",
    "\n",
    "\n",
    "# Calculate the number of clients in each group\n",
    "top_100_clients_count = client_revenue_sorted.head(100).count()\n",
    "top_1_percent_count = int(len(client_revenue_sorted) * 0.01)\n",
    "top_10_percent_count = int(len(client_revenue_sorted) * 0.10)\n",
    "top_25_percent_count = int(len(client_revenue_sorted) * 0.25)\n",
    "top_50_percent_count = int(len(client_revenue_sorted) * 0.50)\n",
    "top_75_percent_count = int(len(client_revenue_sorted) * 0.75)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate revenue for each group\n",
    "top_100_clients_revenue = client_revenue_sorted.head(100).sum()\n",
    "top_1_percent_revenue = client_revenue_sorted.head(top_1_percent_count).sum()\n",
    "top_10_percent_revenue = client_revenue_sorted.head(top_10_percent_count).sum()\n",
    "top_25_percent_revenue = client_revenue_sorted.head(top_25_percent_count).sum()\n",
    "top_50_percent_revenue = client_revenue_sorted.head(top_50_percent_count).sum()\n",
    "top_75_percent_revenue = client_revenue_sorted.head(top_75_percent_count).sum()\n",
    "\n",
    "# Create the table\n",
    "client_repartition_df = pd.DataFrame({\n",
    "    \"Clients\": [\"Top 100 Clients\", \"Top 1%\", \"Top 10%\", \"Top 25%\", \"Top 50%\", \"Top 75%\", \"All Clients\"],\n",
    "        \"Number of Clients\": [\n",
    "        top_100_clients_count, \n",
    "        top_1_percent_count, \n",
    "        top_10_percent_count, \n",
    "        top_25_percent_count, \n",
    "        top_50_percent_count, \n",
    "        top_75_percent_count, \n",
    "        total_clients\n",
    "    ],\n",
    "    \"% of Revenue\": [\n",
    "        f\"{round((top_100_clients_revenue / total_revenue) * 100, 1)}%\", \n",
    "        f\"{round((top_1_percent_revenue / total_revenue) * 100, 1)}%\", \n",
    "        f\"{round((top_10_percent_revenue / total_revenue) * 100, 1)}%\", \n",
    "        f\"{round((top_25_percent_revenue / total_revenue) * 100, 1)}%\", \n",
    "        f\"{round((top_50_percent_revenue / total_revenue) * 100, 1)}%\", \n",
    "        f\"{round((top_75_percent_revenue / total_revenue) * 100, 1)}%\", \n",
    "        \"100%\"\n",
    "    ],\n",
    "    \"Revenue\": [\n",
    "        f\"${top_100_clients_revenue:,.0f}\", \n",
    "        f\"${top_1_percent_revenue:,.0f}\", \n",
    "        f\"${top_10_percent_revenue:,.0f}\", \n",
    "        f\"${top_25_percent_revenue:,.0f}\", \n",
    "        f\"${top_50_percent_revenue:,.0f}\", \n",
    "        f\"${top_75_percent_revenue:,.0f}\", \n",
    "        f\"${total_revenue:,.0f}\"\n",
    "    ],\n",
    "\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "client_repartition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 4. TOP 100 CLIENTS REVENUE CHART --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Group by customer and sum the revenue (Grand Total)\n",
    "client_revenue = data_m2_noCancel_noRefund.groupby('user_id')['revenue_converted'].sum()\n",
    "\n",
    "# Sort by revenue in descending order\n",
    "client_revenue = client_revenue.sort_values(ascending=False)\n",
    "\n",
    "# Limit the data to top 100 clients\n",
    "top_100_revenue = client_revenue.head(100)\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig_top_client_revenue = go.Figure()\n",
    "\n",
    "# Use a bar chart instead of a line chart\n",
    "fig_top_client_revenue.add_trace(go.Bar(\n",
    "    x=list(range(1, len(top_100_revenue) + 1)),  \n",
    "    y=top_100_revenue,  \n",
    "    name='Revenue by Client',\n",
    "    marker=dict(color='lightblue')  \n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_top_client_revenue.update_layout(\n",
    "    title='Revenue for Top 100 Clients',\n",
    "    xaxis_title='Clients (Ordered by Revenue)',\n",
    "    yaxis_title='Revenue',\n",
    "    yaxis=dict(tickprefix='$'),  \n",
    "    template='plotly_white',\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig_top_client_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 5. TOP 100 CLIENTS TRANSACTIONS CHART --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Group by customer and sum the revenue (Grand Total)\n",
    "client_transaction = data_m2_noCancel_noRefund.groupby('user_id')['transaction_id'].count()\n",
    "\n",
    "# Sort by revenue in descending order\n",
    "client_transaction = client_transaction.sort_values(ascending=False)\n",
    "\n",
    "# Limit the data to top 100 clients\n",
    "top_100_transaction = client_transaction.head(100)\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig_top_client_transactions = go.Figure()\n",
    "\n",
    "# Use a bar chart instead of a line chart\n",
    "fig_top_client_transactions.add_trace(go.Bar(\n",
    "    x=list(range(1, len(top_100_transaction) + 1)), \n",
    "    y=top_100_transaction,  \n",
    "    name='Number of Transactions by Client',\n",
    "    marker=dict(color='lightgreen')  \n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_top_client_transactions.update_layout(\n",
    "    title='Number of Transactions for Top 100 Clients',\n",
    "    xaxis_title='Clients (Ordered by Number of Transactions)',\n",
    "    yaxis_title='Total Number of Transactions',\n",
    "    yaxis=dict(tickformat=\"d\"),  # Ensure the y-axis shows integer values\n",
    "    template='plotly_white',\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig_top_client_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 6. MAIN KPIs TOP 1% CLIENTS VS BOTTOM 99% --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False)\n",
    "    & (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Convert 'revenue_converted' => numeric\n",
    "data_m2_noCancel_noRefund['Grand Total (Base)'] = pd.to_numeric(\n",
    "    data_m2_noCancel_noRefund['revenue_converted'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Sort for consistent \"first\" purchase identification\n",
    "data_m2_noCancel_noRefund = data_m2_noCancel_noRefund.sort_values(by=['user_id', 'datetime'])\n",
    "\n",
    "# BUILD CUSTOMER-LEVEL TABLE\n",
    "client_data = data_m2_noCancel_noRefund.groupby('user_id').agg({\n",
    "    'Grand Total (Base)': 'sum',\n",
    "    'transaction_id': 'nunique',\n",
    "    'revenue_converted': 'first'\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "client_data.rename(columns={\n",
    "    'transaction_id': '# of Orders',\n",
    "    'revenue_converted': 'First Purchase Value'\n",
    "}, inplace=True)\n",
    "\n",
    "# Sort by total revenue descending\n",
    "client_data = client_data.sort_values('Grand Total (Base)', ascending=False)\n",
    "\n",
    "# Identify Top 1% and Bottom 99% clients\n",
    "top_1_percent_count = int(len(client_data) * 0.01)\n",
    "top_1_percent_clients = client_data.head(top_1_percent_count)\n",
    "bottom_99_percent_clients = client_data.tail(len(client_data) - top_1_percent_count)\n",
    "top_1_percent_user_ids = top_1_percent_clients.index\n",
    "bottom_99_percent_user_ids = bottom_99_percent_clients.index\n",
    "\n",
    "# SEPARATE DATA FOR METRICS\n",
    "# Data with canceled/refunded orders\n",
    "data_top_1_percent = data_m2[data_m2['user_id'].isin(top_1_percent_user_ids)].copy()\n",
    "data_bottom_99_percent = data_m2[data_m2['user_id'].isin(bottom_99_percent_user_ids)].copy()\n",
    "\n",
    "# Data without canceled/refunded orders\n",
    "data_top_1_percent_clean = data_m2_noCancel_noRefund[\n",
    "    data_m2_noCancel_noRefund['user_id'].isin(top_1_percent_user_ids)\n",
    "].copy()\n",
    "data_bottom_99_percent_clean = data_m2_noCancel_noRefund[\n",
    "    data_m2_noCancel_noRefund['user_id'].isin(bottom_99_percent_user_ids)\n",
    "].copy()\n",
    "\n",
    "def safe_div(numerator, denominator):\n",
    "    \"\"\"Avoid ZeroDivisionError; return 0 if denominator=0.\"\"\"\n",
    "    return numerator / denominator if denominator else 0\n",
    "\n",
    "# Calculate Cancellation and Refund Rates\n",
    "def calculate_rates(data):\n",
    "    cancellations = data[data['order_status'].str.contains('cancel', case=False, na=False)]\n",
    "    refunds = data[data['order_status'].str.contains('refund', case=False, na=False)]\n",
    "    return {\n",
    "        'cancellation_rate': 100 * safe_div(len(cancellations), len(data)),\n",
    "        'refund_rate': 100 * safe_div(len(refunds), len(data))\n",
    "    }\n",
    "\n",
    "rates_top_1_percent = calculate_rates(data_top_1_percent)\n",
    "rates_bottom_99_percent = calculate_rates(data_bottom_99_percent)\n",
    "\n",
    "# Calculate AOV\n",
    "def calculate_aov(data):\n",
    "    total_revenue = data['revenue_converted'].sum()\n",
    "    total_orders = data['transaction_id'].nunique()\n",
    "    return safe_div(total_revenue, total_orders)\n",
    "\n",
    "aov_top_1_percent = calculate_aov(data_top_1_percent_clean)\n",
    "aov_bottom_99_percent = calculate_aov(data_bottom_99_percent_clean)\n",
    "\n",
    "# Calculate Avg Items per Order\n",
    "data_ga4_top_1_percent = data_ga4_item[data_ga4_item['user_id'].isin(top_1_percent_user_ids)]\n",
    "data_ga4_bottom_99_percent = data_ga4_item[data_ga4_item['user_id'].isin(bottom_99_percent_user_ids)]\n",
    "\n",
    "avg_items_per_order_top_1_percent = safe_div(data_ga4_top_1_percent['item_quantity'].sum() , len(data_ga4_top_1_percent['transaction_id'].unique()))\n",
    "avg_items_per_order_bottom_99_percent = safe_div(data_ga4_bottom_99_percent['item_quantity'].sum() , len(data_ga4_bottom_99_percent['transaction_id'].unique()))\n",
    "\n",
    "# Calculate Days Between Orders\n",
    "def calculate_avg_days_between_orders(data):\n",
    "    data_sorted = data.sort_values(['user_id', 'datetime'])\n",
    "    data_sorted['days_between_orders'] = data_sorted.groupby('user_id')['datetime'].diff().dt.days\n",
    "    return data_sorted['days_between_orders'].mean()\n",
    "\n",
    "days_between_orders_top_1_percent = calculate_avg_days_between_orders(data_top_1_percent_clean)\n",
    "days_between_orders_bottom_99_percent = calculate_avg_days_between_orders(data_bottom_99_percent_clean)\n",
    "\n",
    "def safe_percentage(value):\n",
    "    \"\"\"Format a numeric value as a percentage string, handling NaN or inf.\"\"\"\n",
    "    if pd.isna(value) or np.isinf(value):\n",
    "        return \"\"\n",
    "    return f\"{value:.1f}%\"\n",
    "\n",
    "# Create comparison DataFrame\n",
    "top_1_percent_metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Number of Clients\",\n",
    "        \"Average # of Orders\",\n",
    "        \"Min / Max # of Orders\",\n",
    "        \"Average LTV\",\n",
    "        \"Min / Max LTV\",\n",
    "        \"Average First Purchase Value\",\n",
    "        \"Min / Max First Purchase Value\",\n",
    "        \"Rate of Cancellations (%)\",\n",
    "        \"Rate of Refunds (%)\",\n",
    "        \"Average Order Value (AOV)\",\n",
    "        \"Average Number of Items per Order\",\n",
    "        \"Average Number of Days Between Orders\"\n",
    "    ],\n",
    "    \"Top 1% Clients\": [\n",
    "        f\"{len(top_1_percent_clients):,.0f}\",\n",
    "        f\"{top_1_percent_clients['# of Orders'].mean():.1f}\",\n",
    "        f\"{top_1_percent_clients['# of Orders'].min()} / {top_1_percent_clients['# of Orders'].max()}\",\n",
    "        f\"${top_1_percent_clients['Grand Total (Base)'].mean():,.1f}\",\n",
    "        f\"${top_1_percent_clients['Grand Total (Base)'].min():,.1f} / ${top_1_percent_clients['Grand Total (Base)'].max():,.1f}\",\n",
    "        f\"${top_1_percent_clients['First Purchase Value'].mean():,.1f}\",\n",
    "        f\"${top_1_percent_clients['First Purchase Value'].min():,.1f} / ${top_1_percent_clients['First Purchase Value'].max():,.1f}\",\n",
    "        safe_percentage(rates_top_1_percent['cancellation_rate']),\n",
    "        safe_percentage(rates_top_1_percent['refund_rate']),\n",
    "        f\"${aov_top_1_percent:,.1f}\",\n",
    "        f\"{avg_items_per_order_top_1_percent:.1f}\",\n",
    "        f\"{days_between_orders_top_1_percent:.1f}\"\n",
    "    ],\n",
    "    \"Bottom 99% Clients\": [\n",
    "        f\"{len(bottom_99_percent_clients):,.0f}\",\n",
    "        f\"{bottom_99_percent_clients['# of Orders'].mean():.1f}\",\n",
    "        f\"{bottom_99_percent_clients['# of Orders'].min()} / {bottom_99_percent_clients['# of Orders'].max()}\",\n",
    "        f\"${bottom_99_percent_clients['Grand Total (Base)'].mean():,.1f}\",\n",
    "        f\"${bottom_99_percent_clients['Grand Total (Base)'].min():,.1f} / ${bottom_99_percent_clients['Grand Total (Base)'].max():,.1f}\",\n",
    "        f\"${bottom_99_percent_clients['First Purchase Value'].mean():,.1f}\",\n",
    "        f\"${bottom_99_percent_clients['First Purchase Value'].min():,.1f} / ${bottom_99_percent_clients['First Purchase Value'].max():,.1f}\",\n",
    "        safe_percentage(rates_bottom_99_percent['cancellation_rate']),\n",
    "        safe_percentage(rates_bottom_99_percent['refund_rate']),\n",
    "        f\"${aov_bottom_99_percent:,.1f}\",\n",
    "        f\"{avg_items_per_order_bottom_99_percent:.1f}\",\n",
    "        f\"{days_between_orders_bottom_99_percent:.1f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display final comparison\n",
    "top_1_percent_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 7. CORRELATION ANALYSIS --\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Ensure numeric data\n",
    "data_m2_noCancel_noRefund['Grand Total (Base)'] = pd.to_numeric(data_m2_noCancel_noRefund['revenue_converted'], errors='coerce')\n",
    "\n",
    "# Calculate the first purchase value for each customer\n",
    "data_m2_noCancel_noRefund['First Purchase Value'] = data_m2_noCancel_noRefund.groupby('user_id')['Grand Total (Base)'].transform('first')\n",
    "\n",
    "# Group by customer to calculate total and first purchase revenues\n",
    "client_data = data_m2_noCancel_noRefund.groupby('user_id').agg({\n",
    "    'Grand Total (Base)': 'sum',  # Lifetime Value (LTV)\n",
    "    'First Purchase Value': 'first'  # First Purchase Value (FPV)\n",
    "}).rename(columns={'Grand Total (Base)': 'LTV', 'First Purchase Value': 'FPV'})\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation, p_value = pearsonr(client_data['FPV'], client_data['LTV'])\n",
    "\n",
    "# Find illustrative examples\n",
    "high_ltv_examples = client_data.sort_values('LTV', ascending=False).head(5)\n",
    "\n",
    "correlation_results_df = pd.DataFrame({\n",
    "    'Metric': ['Correlation Coefficient (r)', 'P-value', 'Statistical Significance'],\n",
    "    'Value': [\n",
    "        f\"{correlation:.4f}\",\n",
    "        f\"{p_value:.4e}\",\n",
    "        \"Statistically Significant\" if p_value < 0.05 else \"Not Statistically Significant\"\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Pearson correlation between First Purchase Value and Lifetime Value',\n",
    "        'Probability that the correlation occurred by chance',\n",
    "        'Significant if p-value < 0.05'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Output results\n",
    "correlation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 8. FIRST PURCHASE VS. TOTAL REVENUE --\n",
    "\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Ensure numeric data\n",
    "data_m2_noCancel_noRefund['Grand Total (Base)'] = pd.to_numeric(data_m2_noCancel_noRefund['revenue_converted'], errors='coerce')\n",
    "\n",
    "# Calculate the first purchase value for each customer\n",
    "data_m2_noCancel_noRefund['First Purchase Value'] = data_m2_noCancel_noRefund.groupby('user_id')['Grand Total (Base)'].transform('first')\n",
    "\n",
    "# Group by customer to calculate total and first purchase revenues\n",
    "client_data = data_m2_noCancel_noRefund.groupby('user_id').agg({\n",
    "    'Grand Total (Base)': 'sum',\n",
    "    'First Purchase Value': 'first'\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "total_revenue = client_data['Grand Total (Base)'].sum()\n",
    "first_purchase_revenue = client_data['First Purchase Value'].sum()\n",
    "revenue_less_first_purchase = total_revenue - first_purchase_revenue\n",
    "\n",
    "# Create a Plotly bar chart\n",
    "fig_first_purchase_vs_revenue = go.Figure()\n",
    "\n",
    "# Add bars for first purchase revenue and remaining revenue\n",
    "fig_first_purchase_vs_revenue.add_trace(go.Bar(\n",
    "    x=[\"First Purchase Revenue\", \"All Revenue less First Purchase\"],\n",
    "    y=[first_purchase_revenue / total_revenue * 100, revenue_less_first_purchase / total_revenue * 100],\n",
    "    text=[f\"{first_purchase_revenue / total_revenue * 100:.1f}%\", f\"{revenue_less_first_purchase / total_revenue * 100:.1f}%\"],  # Rounded to 1 decimal place\n",
    "    textposition='auto',\n",
    "    marker=dict(color=['blue', 'lightblue']),\n",
    "    width=0.35  # Reduce the width of the bars\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_first_purchase_vs_revenue.update_layout(\n",
    "    title=\"First Purchase vs. Total Revenue\",\n",
    "    xaxis_title=\"\",  # Remove the x-axis title\n",
    "    yaxis_title=\"Percentage of Total Revenue\",\n",
    "    yaxis=dict(ticksuffix=\"%\"),\n",
    "    template=\"plotly_white\",\n",
    "    width=700,\n",
    "    height=500,\n",
    "    bargap=0.3,  # Add some gap between the bars\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "fig_first_purchase_vs_revenue.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 9. FIRST PURCHASE VS. TOTAL REVENUE --\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "# Ensure numeric data\n",
    "data_m2_noCancel_noRefund['Grand Total (Base)'] = pd.to_numeric(data_m2_noCancel_noRefund['revenue_converted'], errors='coerce')\n",
    "\n",
    "# Calculate the first purchase value for each customer\n",
    "data_m2_noCancel_noRefund['First Purchase Value'] = data_m2_noCancel_noRefund.groupby('user_id')['Grand Total (Base)'].transform('first')\n",
    "\n",
    "# Group by customer to calculate total and first purchase revenues\n",
    "client_data = data_m2_noCancel_noRefund.groupby('user_id').agg({\n",
    "    'Grand Total (Base)': 'sum',\n",
    "    'First Purchase Value': 'first'\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "total_revenue = client_data['Grand Total (Base)'].sum()\n",
    "first_purchase_revenue = client_data['First Purchase Value'].sum()\n",
    "revenue_less_first_purchase = total_revenue - first_purchase_revenue\n",
    "\n",
    "# Calculate percentages\n",
    "first_purchase_percentage = (first_purchase_revenue / total_revenue) * 100\n",
    "remaining_revenue_percentage = (revenue_less_first_purchase / total_revenue) * 100\n",
    "\n",
    "# Create the table as a pandas DataFrame\n",
    "first_purchase_vs_total_revenue_df = pd.DataFrame({\n",
    "    \"Data\": [\"First Purchase Revenue\", \"All Revenue - First Purchase\"],\n",
    "    \"% of Revenue\": [f\"{first_purchase_percentage:.1f}%\", f\"{remaining_revenue_percentage:.1f}%\"],\n",
    "    \"Revenue\": [f\"${first_purchase_revenue:.0f}\", f\"${revenue_less_first_purchase:.0f}\"]\n",
    "})\n",
    "\n",
    "\n",
    "# Display the table\n",
    "first_purchase_vs_total_revenue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by user origin (traffic_source) and aggregate revenue and quantity\n",
    "grouped = data_ga4_source.groupby(\"traffic_source\", as_index=False).agg({\n",
    "    \"revenue\": \"sum\",\n",
    "    \"quantity\": \"sum\"\n",
    "})\n",
    "\n",
    "# Rename 'traffic_source' to 'User Origin'\n",
    "grouped.rename(columns={\"traffic_source\": \"User Origin\"}, inplace=True)\n",
    "\n",
    "# Calculate total revenue and total items purchased\n",
    "total_revenue = grouped[\"revenue\"].sum()\n",
    "total_items = grouped[\"quantity\"].sum()\n",
    "\n",
    "# Calculate percentage metrics\n",
    "grouped[\"% of Revenue\"] = grouped[\"revenue\"] / total_revenue * 100\n",
    "grouped[\"% of Items Purchased\"] = grouped[\"quantity\"] / total_items * 100\n",
    "\n",
    "# Optionally, sort by revenue and select the top 10 user origins\n",
    "grouped.sort_values(by=\"revenue\", ascending=False, inplace=True)\n",
    "top10 = grouped.head(10)\n",
    "\n",
    "# Reshape data for plotting\n",
    "# Convert the dataframe from wide to long format for plotting\n",
    "top10_long = top10.melt(\n",
    "    id_vars=[\"User Origin\"],\n",
    "    value_vars=[\"% of Revenue\", \"% of Items Purchased\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Percentage\"\n",
    ")\n",
    "\n",
    "# Create the grouped bar chart with specified colors and legend position\n",
    "# Define a color mapping for the metrics\n",
    "color_map = {\n",
    "    \"% of Revenue\": \"lightblue\",\n",
    "    \"% of Items Purchased\": \"lightgreen\"\n",
    "}\n",
    "\n",
    "fig_user_origin_chart = px.bar(\n",
    "    top10_long,\n",
    "    x=\"User Origin\",\n",
    "    y=\"Percentage\",\n",
    "    color=\"Metric\",\n",
    "    barmode=\"group\",  # places bars side-by-side\n",
    "    title=\"User Origin Metrics: % of Revenue vs. % of Items Purchased\",\n",
    "    labels={\"Percentage\": \"Percentage (%)\"},\n",
    "    color_discrete_map=color_map  # apply our custom colors\n",
    ")\n",
    "\n",
    "# Update layout to place the legend at the top center, remove x-axis title and \"Metric\" from the legend\n",
    "fig_user_origin_chart.update_layout(\n",
    "    xaxis_title='',  # Remove the x-axis title\n",
    "    legend_title='',  # Remove the legend title (\"Metric\")\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig_user_origin_chart.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- 11. USER ORIGIN TABLE --\n",
    "\n",
    "# Group by user origin (traffic_source) and aggregate revenue and quantity\n",
    "grouped = data_ga4_source.groupby(\"traffic_source\", as_index=False).agg({\n",
    "    \"revenue\": \"sum\",\n",
    "    \"quantity\": \"sum\"\n",
    "})\n",
    "\n",
    "# Rename 'traffic_source' to 'User Origin'\n",
    "grouped.rename(columns={\"traffic_source\": \"User Origin\"}, inplace=True)\n",
    "\n",
    "# Calculate totals\n",
    "total_revenue = grouped[\"revenue\"].sum()\n",
    "total_items = grouped[\"quantity\"].sum()\n",
    "\n",
    "# Calculate percentages and average revenue\n",
    "grouped[\"% of Revenue\"] = grouped[\"revenue\"] / total_revenue * 100\n",
    "grouped[\"% of Items Purchased\"] = grouped[\"quantity\"] / total_items * 100\n",
    "grouped[\"Average Revenue per Item\"] = grouped[\"revenue\"] / grouped[\"quantity\"]\n",
    "\n",
    "# Replace NaN/inf values (e.g., where quantity = 0)\n",
    "grouped[\"Average Revenue per Item\"] = grouped[\"Average Revenue per Item\"].fillna(0).replace([float('inf'), -float('inf')], 0)\n",
    "\n",
    "# Format columns\n",
    "grouped[\"% of Revenue\"] = grouped[\"% of Revenue\"].map(\"{:.1f}%\".format)\n",
    "grouped[\"% of Items Purchased\"] = grouped[\"% of Items Purchased\"].map(\"{:.1f}%\".format)\n",
    "grouped[\"Average Revenue per Item\"] = grouped[\"Average Revenue per Item\"].map(\"${:,.1f}\".format)\n",
    "\n",
    "# Sort by total revenue desc and keep top 10\n",
    "grouped.sort_values(by=\"revenue\", ascending=False, inplace=True)\n",
    "top10 = grouped.head(10)\n",
    "\n",
    "# Create final table (you can include 'revenue' and 'quantity' columns too if needed)\n",
    "client_origin_df = top10[[\n",
    "    \"User Origin\",\n",
    "    \"% of Revenue\",\n",
    "    \"% of Items Purchased\",\n",
    "    \"Average Revenue per Item\"\n",
    "]]\n",
    "\n",
    "client_origin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 12. PRODUCTS FOR TOP 10% CUSTOMERS --\n",
    "\n",
    "\"\"\"\n",
    "Example of comment:\n",
    "Among the top 10% of customers, 40% began their journey with premium sneakers, 30% with sportswear bundles, and 20% with high-value accessories.\n",
    "\"\"\"\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_ga4_item_noCancel_noRefund = data_ga4_item[\n",
    "    ~data_ga4_item['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_ga4_item['revenue'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Group by customer to get total revenue\n",
    "customer_revenue = (\n",
    "    data_ga4_item_noCancel_noRefund.groupby('user_id', as_index=False)['revenue']\n",
    "      .sum()\n",
    "      .rename(columns={'revenue': 'total_revenue'})\n",
    ")\n",
    "\n",
    "# Calculate the revenue threshold for top 10%\n",
    "threshold_10pct = customer_revenue['total_revenue'].quantile(0.90)\n",
    "\n",
    "# Get the list (subset) of top 10% customers\n",
    "top_10pct_customers = customer_revenue[\n",
    "    customer_revenue['total_revenue'] >= threshold_10pct\n",
    "]['user_id']\n",
    "\n",
    "# -- 2. Find each top-10%-customer's first purchase category\n",
    "# We'll only consider rows from those top 10% customers\n",
    "df_top_10pct = data_ga4_item_noCancel_noRefund[data_ga4_item_noCancel_noRefund['user_id'].isin(top_10pct_customers)].copy()\n",
    "\n",
    "# Sort by date to identify first purchase\n",
    "df_top_10pct = df_top_10pct.sort_values(by=['user_id', 'datetime_x'])\n",
    "\n",
    "# Drop duplicates so each customer is kept at his/her earliest purchase record\n",
    "df_first_purchase = (\n",
    "    df_top_10pct\n",
    "    .drop_duplicates(subset=['user_id'], keep='first')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# --3. Calculate the percentage share of each first-purchase category\n",
    "category_counts = (\n",
    "    df_first_purchase.groupby(category_to_analyse)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "\n",
    "total_first_purchases = category_counts['count'].sum()\n",
    "category_counts['percentage'] = (category_counts['count'] / total_first_purchases) * 100\n",
    "\n",
    "# -- 4. Plot horizontal bar chart with Plotly\n",
    "fig_category_percentage_chart = px.bar(\n",
    "    category_counts,\n",
    "    x='percentage',               # numeric axis\n",
    "    y=category_to_analyse,            # category axis\n",
    "    orientation='h',             # horizontal bars\n",
    "    color=category_to_analyse,        # color by category (optional)\n",
    "    title='Distribution of First Purchase Category (Top 10% Customers)',\n",
    "    labels={'percentage': '% of First Purchases', category_to_analyse: 'Category'}\n",
    ")\n",
    "\n",
    "fig_category_percentage_chart.update_traces(marker_color='lightgreen')  # Change 'blue' to any desired color\n",
    "\n",
    "\n",
    "# Make layout adjustments for readability\n",
    "fig_category_percentage_chart.update_layout(\n",
    "    showlegend=False,  # Hide legend if categories are self-explanatory\n",
    "    xaxis_tickformat=\".0f\",  # Format percentages\n",
    "    yaxis={'categoryorder':'total ascending'},  # Largest bar at top\n",
    "    bargap=0.3,\n",
    "    plot_bgcolor=\"white\",  # Set plot background to white\n",
    "    paper_bgcolor=\"white\"  # Set overall figure background to white\n",
    ")\n",
    "\n",
    "fig_category_percentage_chart.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 13. SUMMARY AND CORRELATION CATEGORY ANALYSIS --\n",
    "\n",
    "\"\"\"\n",
    "Example of comment:\n",
    "The highest positive correlation is about 0.23 for \"Products/FPGA Boards/Expansion Modules/Pmods\". That is a mild positive relationship with LTV. It suggests that customers who spend more on that category tend to have higher LTV, but its not a very strong correlation.\n",
    "Many other categories show small (or near-zero) correlations, meaning they dont appear to strongly drive LTV in a linear sense.\n",
    "A high transaction count + low correlation likely means that category is universal or low-value. Many customers buy it regardless of whether they go on to become high-LTV or low-LTV.\n",
    "A smaller transaction count + high positive correlation might indicate a specialized product that predicts or drives higher overall spend.\n",
    "\"\"\"\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_ga4_item_noCancel_noRefund = data_ga4_item[\n",
    "    ~data_ga4_item['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), \n",
    "                                                case=False, na=False) &\n",
    "    (data_ga4_item['revenue'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Group by customer to get total revenue\n",
    "customer_revenue = (\n",
    "    data_ga4_item_noCancel_noRefund.groupby('user_id', as_index=False)['revenue']\n",
    "      .sum()\n",
    "      .rename(columns={'revenue': 'total_revenue'})\n",
    ")\n",
    "\n",
    "# Calculate the revenue threshold for top 10%\n",
    "threshold_10pct = customer_revenue['total_revenue'].quantile(0.90)\n",
    "\n",
    "# Get the list (subset) of top 10% customers\n",
    "top_10pct_customers = customer_revenue[\n",
    "    customer_revenue['total_revenue'] >= threshold_10pct\n",
    "]['user_id']\n",
    "\n",
    "# -- 2. Find each top-10%-customer's first purchase category\n",
    "# We'll only consider rows from those top 10% customers\n",
    "df_top_10pct = data_ga4_item_noCancel_noRefund[\n",
    "    data_ga4_item_noCancel_noRefund['user_id'].isin(top_10pct_customers)\n",
    "].copy()\n",
    "\n",
    "# Identify each user's FIRST purchase (among top 10% only)\n",
    "df_sorted = df_top_10pct.sort_values(by=['user_id', 'datetime_x'])\n",
    "df_first_purchase = df_sorted.drop_duplicates(subset=['user_id'], keep='first').copy()\n",
    "\n",
    "# We'll focus on category_to_analyse\n",
    "df_first_purchase = df_first_purchase[['user_id', category_to_analyse]].copy()\n",
    "\n",
    "# Create a onehot encoding for firstpurchase categories\n",
    "df_one_hot = pd.get_dummies(\n",
    "    df_first_purchase[['user_id', category_to_analyse]],\n",
    "    columns=[category_to_analyse],\n",
    "    prefix='firstCat',\n",
    "    dtype=int\n",
    ")\n",
    "df_one_hot.set_index('user_id', inplace=True)\n",
    "\n",
    "# Get total LTV by user (top 10% only)\n",
    "df_ltv = (\n",
    "    df_top_10pct.groupby('user_id', as_index=False)['revenue']\n",
    "               .sum()\n",
    "               .rename(columns={'revenue': 'LTV'})\n",
    ")\n",
    "\n",
    "# Merge onehot table with LTV table\n",
    "df_merged = df_one_hot.merge(df_ltv, left_index=True, right_on='user_id', how='left')\n",
    "\n",
    "# Correlation of firstpurchase category with LTV\n",
    "df_merged_numeric = df_merged.drop(columns=['user_id']).select_dtypes(['int', 'float'])\n",
    "corr_matrix = df_merged_numeric.corr()\n",
    "corr_with_ltv = corr_matrix['LTV'].drop('LTV')  # exclude LTV row itself\n",
    "\n",
    "# Count how many users had each firstpurchase category (in top 10%)\n",
    "df_cat_counts = (\n",
    "    df_first_purchase.groupby(category_to_analyse)\n",
    "                     .size()\n",
    "                     .reset_index(name='num_users')\n",
    ")\n",
    "total_first_purchases = df_cat_counts['num_users'].sum()\n",
    "df_cat_counts['pct_total'] = (df_cat_counts['num_users'] / total_first_purchases) * 100\n",
    "\n",
    "# Convert correlation result into DataFrame & map dummy columns back\n",
    "corr_df = corr_with_ltv.to_frame(name='correlation_with_ltv').reset_index()\n",
    "corr_df[category_to_analyse] = corr_df['index'].str.replace('^firstCat_', '', regex=True)\n",
    "\n",
    "category_repartition_df = pd.merge(\n",
    "    df_cat_counts,\n",
    "    corr_df,\n",
    "    on=category_to_analyse,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename columns to highlight Top 10% context\n",
    "category_repartition_df.rename(columns={\n",
    "    category_to_analyse: 'Category Name',\n",
    "    'num_users': 'Num First Purchases (Top 10%)',\n",
    "    'pct_total': '% of First Purchases (Top 10%)',\n",
    "    'correlation_with_ltv': 'Correlation with LTV (Top 10%)'\n",
    "}, inplace=True)\n",
    "\n",
    "# Sort and tidy\n",
    "category_repartition_df.sort_values(by='Num First Purchases (Top 10%)', ascending=False, inplace=True)\n",
    "category_repartition_df['% of First Purchases (Top 10%)'] = (category_repartition_df['% of First Purchases (Top 10%)'].round(1)).astype(str) + '%'\n",
    "category_repartition_df['Correlation with LTV (Top 10%)'] = category_repartition_df['Correlation with LTV (Top 10%)'].round(2)\n",
    "\n",
    "# Final summary\n",
    "category_repartition_df = category_repartition_df[['Category Name',\n",
    "            '% of First Purchases (Top 10%)', 'Correlation with LTV (Top 10%)']].head(10)\n",
    "\n",
    "category_repartition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 14. Ratio discount over time --\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "# Here we designate an order as 'Discounted' if it has a discount value > 0 or a discount code exists.\n",
    "data_m2_noCancel_noRefund['order_type'] = data_m2_noCancel_noRefund.apply(\n",
    "    lambda row: 'Discounted' if (row['discount'] > 0) else 'Full Price',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create a new column 'order_month' by extracting the month from 'datetime'\n",
    "# The .to_period('M') converts to a monthly period, and .to_timestamp() converts it back to a datetime for plotting.\n",
    "data_m2_noCancel_noRefund['order_month'] = data_m2_noCancel_noRefund['datetime'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Group the data by order_month and order_type to get monthly order counts\n",
    "monthly_trend = data_m2_noCancel_noRefund.groupby(['order_month', 'order_type']).agg(\n",
    "    order_count=('transaction_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to have separate columns for each order types count per month\n",
    "monthly_count_pivot = monthly_trend.pivot(index='order_month', columns='order_type', values='order_count').fillna(0)\n",
    "\n",
    "# --- Calculate the Discount Ratio ---\n",
    "# Ensure that both columns exist; if one of the types is missing for a month, fill with 0.\n",
    "# Ratio = (Discounted orders) / (Discounted orders + Full Price orders)\n",
    "monthly_count_pivot['discount_ratio'] = (\n",
    "    monthly_count_pivot.get('Discounted', 0) / \n",
    "    (monthly_count_pivot.get('Discounted', 0) + monthly_count_pivot.get('Full Price', 0))\n",
    ")\n",
    "\n",
    "# --- Generate a Line Chart for the Monthly Discount Ratio ---\n",
    "fig_ratio_discount = go.Figure()\n",
    "\n",
    "fig_ratio_discount.add_trace(go.Scatter(\n",
    "    x=monthly_count_pivot.index,\n",
    "    y=monthly_count_pivot['discount_ratio'],\n",
    "    mode='lines',\n",
    "    name='Discount Ratio',\n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_ratio_discount.update_layout(\n",
    "    title='Monthly Trend of Ratio: Discounted Transactions / Total Transactions',\n",
    "    yaxis_title='Discount Ratio',\n",
    "    yaxis=dict(tickformat='.0%'), \n",
    "    template='plotly_white',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_ratio_discount.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 15. Customer Segmentation Discount --\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "# --- Prepare the data ---\n",
    "# Create a new column 'order_type' that designates whether an order was discounted.\n",
    "data_m2_noCancel_noRefund['order_type'] = data_m2_noCancel_noRefund.apply(\n",
    "    lambda row: 'Discounted' if (row['discount'] > 0) else 'Full Price', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group the data by order_type to calculate:\n",
    "# - The count of orders\n",
    "# - Total revenue\n",
    "order_summary = data_m2_noCancel_noRefund.groupby('order_type').agg(\n",
    "    order_count=('transaction_id', 'count'),\n",
    "    total_revenue=('revenue_converted', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate total revenue and total transactions\n",
    "total_revenue = order_summary['total_revenue'].sum()\n",
    "total_orders = order_summary['order_count'].sum()\n",
    "\n",
    "# Calculate percentage metrics\n",
    "order_summary['% of Revenue'] = (order_summary['total_revenue'] / total_revenue) * 100\n",
    "order_summary['% of Transactions'] = (order_summary['order_count'] / total_orders) * 100\n",
    "\n",
    "# Reshape the data for plotting\n",
    "# Convert the dataframe from wide to long format for plotting\n",
    "order_summary_long = order_summary.melt(\n",
    "    id_vars=[\"order_type\"],\n",
    "    value_vars=[\"% of Revenue\", \"% of Transactions\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Percentage\"\n",
    ")\n",
    "\n",
    "# --- Generate the grouped bar chart ---\n",
    "fig_share_discount = go.Figure()\n",
    "\n",
    "# Define a color mapping for the metrics\n",
    "color_map = {\n",
    "    \"% of Revenue\": \"lightblue\",\n",
    "    \"% of Transactions\": \"lightgreen\"\n",
    "}\n",
    "\n",
    "# Add bars for % of Revenue (left side)\n",
    "fig_share_discount.add_trace(go.Bar(\n",
    "    x=order_summary_long['order_type'],\n",
    "    y=order_summary_long[order_summary_long['Metric'] == \"% of Revenue\"]['Percentage'],\n",
    "    name=\"% of Revenue\",  # Change legend name\n",
    "    marker=dict(color=\"lightblue\"),\n",
    "    offsetgroup=0,  # Ensure this group is on the left side\n",
    "    width=0.35,  # Reduce the bar width\n",
    "    text=order_summary_long[order_summary_long['Metric'] == \"% of Revenue\"]['Percentage'].round(1).astype(str) + '%',  # Add the label text\n",
    "    textposition='outside',  # Position the text outside the bar (top)\n",
    "))\n",
    "\n",
    "# Add bars for % of Transactions (right side)\n",
    "fig_share_discount.add_trace(go.Bar(\n",
    "    x=order_summary_long['order_type'],\n",
    "    y=order_summary_long[order_summary_long['Metric'] == \"% of Transactions\"]['Percentage'],\n",
    "    name=\"% of Transactions\",  # Change legend name\n",
    "    marker=dict(color=\"lightgreen\"),\n",
    "    offsetgroup=1,  # Ensure this group is on the right side\n",
    "    width=0.35,  # Reduce the bar width\n",
    "    text=order_summary_long[order_summary_long['Metric'] == \"% of Transactions\"]['Percentage'].round(1).astype(str) + '%',  # Add the label text\n",
    "    textposition='outside',  # Position the text outside the bar (top)\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig_share_discount.update_layout(\n",
    "    title=\"Discount vs Full Price Transactions\",\n",
    "    yaxis_title=\"Percentage (%)\",\n",
    "    barmode=\"group\",  # places bars side-by-side\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    bargap=0.2,  # Add small gap between bars (adjust value for more or less space)\n",
    "    bargroupgap=0.3  # Adjust gap between groups\n",
    ")\n",
    "\n",
    "fig_share_discount.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 16.Customer Segmentation Discount --\n",
    "# Remove canceled/refunded, keep only revenue > 0\n",
    "data_m2_noCancel_noRefund = data_m2[\n",
    "    ~data_m2['order_status'].str.contains(\"|\".join(order_status_keyWords_Refund_Cancel), case=False, na=False) & \n",
    "    (data_m2['revenue_converted'] > 0)\n",
    "].copy()\n",
    "\n",
    "# Create a new column 'order_type' that designates whether an order was discounted.\n",
    "data_m2_noCancel_noRefund['order_type'] = data_m2_noCancel_noRefund.apply(\n",
    "    lambda row: 'Discounted' if (row['discount'] > 0 ) else 'Full Price', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group orders by customer (using 'user_name' or another unique identifier)\n",
    "customer_discount = data_m2_noCancel_noRefund.groupby('user_id').agg(\n",
    "    total_orders=('transaction_id', 'count'),\n",
    "    discounted_orders=('order_type', lambda x: (x == 'Discounted').sum()),\n",
    "    fullprice_orders=('order_type', lambda x: (x == 'Full Price').sum()),\n",
    "    total_revenue=('revenue_converted', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Classify each customer\n",
    "customer_discount['customer_segment'] = customer_discount.apply(\n",
    "    lambda row: 'Discount Only' if row['discounted_orders'] == row['total_orders'] \n",
    "                else ('Full Price Only' if row['fullprice_orders'] == row['total_orders'] else 'Mixed'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create a summary table by segment\n",
    "customer_segment_discount_df = customer_discount.groupby('customer_segment').agg(\n",
    "    number_of_customers=('user_id', 'count'),\n",
    "    total_revenue=('total_revenue', 'sum'),\n",
    "    average_revenue_per_customer=('total_revenue', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate percentage values manually\n",
    "customer_segment_discount_df['percentage_customers'] = (customer_segment_discount_df['number_of_customers'] / customer_segment_discount_df['number_of_customers'].sum()) * 100\n",
    "\n",
    "# --- Generate a pie chart to visualize customer segmentation ---\n",
    "fig_client_distribution_discount = go.Figure(data=[go.Pie(\n",
    "    labels=customer_segment_discount_df['customer_segment'],\n",
    "    values=customer_segment_discount_df['percentage_customers'],\n",
    "    hole=0.5,  # Increase the hole size to 50%\n",
    "    marker=dict(colors=['#66c2a5', '#fc8d62', '#8da0cb']),  # Custom colors\n",
    "    textinfo='percent',  # Display the percentage\n",
    "    texttemplate='%{value:.1f}%',  # Manually format percentages with 1 decimal place\n",
    ")])\n",
    "\n",
    "fig_client_distribution_discount.update_layout(\n",
    "    title='Discount vs Full Price Buyers',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Render the pie chart\n",
    "fig_client_distribution_discount.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Customer Segmentation Discount Table --\n",
    "\n",
    "# Calculate percentage values manually\n",
    "customer_segment_discount_df['percentage_customers'] = (customer_segment_discount_df['number_of_customers'] / customer_segment_discount_df['number_of_customers'].sum()) * 100\n",
    "\n",
    "# Format values to 1 decimal place for all columns\n",
    "customer_segment_discount_df['percentage_customers'] = customer_segment_discount_df['percentage_customers'].round(1).astype(str) + '%'  # Add '%' sign\n",
    "customer_segment_discount_df['total_revenue'] = '$' + customer_segment_discount_df['total_revenue'].round(1).astype(str)  # Add '$' sign and convert to string\n",
    "customer_segment_discount_df['average_revenue_per_customer'] = '$' + customer_segment_discount_df['average_revenue_per_customer'].round(1).astype(str)  # Add '$' sign and convert to string\n",
    "\n",
    "# Rename columns to something more natural\n",
    "customer_segment_discount_df.rename(columns={\n",
    "    'customer_segment': 'Client Segment',\n",
    "    'number_of_customers': 'Number of Clients',\n",
    "    'percentage_customers': '% of Clients',\n",
    "    'total_revenue': 'Total Revenue',\n",
    "    'average_revenue_per_customer': 'Avg Revenue per Client'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the final summary table\n",
    "customer_segment_discount_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Save the output in folder [datetime]_results --\n",
    "\n",
    "# Get the current date and time formatted as YYYYMMDD_HHMMSS\n",
    "current_datetime = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "# Define the folder path as [datetime]_revenue\n",
    "output_dir = f'{current_datetime}_results/'\n",
    "# Create the results directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "# -- Export all tables to a single Excel file --\n",
    "# Use ExcelWriter to create a file with multiple sheets\n",
    "output_excel_path = os.path.join(output_dir, \"combined_results.xlsx\")\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    # Write each DataFrame to a separate sheet in the Excel file\n",
    "    client_repartition_df.to_excel(writer, sheet_name='client_repartition_df', index=False)\n",
    "    top_1_percent_metrics_df.to_excel(writer, sheet_name='top_1_percent_metrics_df', index=False)\n",
    "    correlation_results_df.to_excel(writer, sheet_name='correlation_results_df', index=False)\n",
    "    first_purchase_vs_total_revenue_df.to_excel(writer, sheet_name='first_purchase_vs_total_revenue_df', index=False)\n",
    "    client_origin_df.to_excel(writer, sheet_name='client_origin_df', index=False)\n",
    "    customer_segment_discount_df.to_excel(writer, sheet_name='customer_segment_discount_df', index=False)\n",
    "    category_repartition_df.to_excel(writer, sheet_name='category_repartition_df', index=False)\n",
    "\n",
    "\n",
    "# Output confirmation\n",
    "print(f\"All tables have been successfully written to {output_excel_path}\")\n",
    "\n",
    "\n",
    "# -- Export all figures to PNG files --\n",
    "figures = {\n",
    "    \"1-revenue_timeline\": fig_revenue_timeline,\n",
    "    \"2-revenue_month_and_weekday\": fig_revenue_distributin_month_and_weekday,\n",
    "    \"3-client_revenue\": fig_client_revenue,  \n",
    "    \"4-top_client_revenue\": fig_top_client_revenue,  \n",
    "    \"5-top_client_transactions\": fig_top_client_transactions,  \n",
    "    \"6-first_purchase_vs_revenue\": fig_first_purchase_vs_revenue, \n",
    "    \"7-user_origin_chart\": fig_user_origin_chart,\n",
    "    \"8-ratio_discount\": fig_ratio_discount, \n",
    "    \"9-share_discountt\": fig_share_discount, \n",
    "    \"10-client_distribution_discount\": fig_client_distribution_discount, \n",
    "    \"11-category_percentage_chart\": fig_category_percentage_chart, \n",
    "}  \n",
    "\n",
    "# Save each figure as a PNG in the /results/ folder\n",
    "for fig_name, fig_obj in figures.items():\n",
    "    fig_file_path = os.path.join(output_dir, f\"{fig_name}.png\")\n",
    "    pio.write_image(fig_obj, fig_file_path)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"All figures have been successfully saved as PNG files in the 'results' folder.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
